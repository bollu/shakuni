{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal probabilistic expression in Haskell\n",
    "### Haskell Kōan: Probabilstic programming with conditioning\n",
    "\n",
    "We implement a tiny embedded domain-specific language which allows us to _sample from random variables_ and\n",
    "build computations from them. We also build the less-known but equally important ability to\n",
    "use _conditional reasoning_, where we can condition a random variable on another random variable. This is not as well known, and implementing this is the raison d'être of this Kōan.\n",
    "\n",
    "Implementing this within haskell with a monadic inteface allows us to leverage the full power of Haskell,\n",
    "making our computations compositional, and our implementation of _conditioning_ concise. We will allow users to build expressions denoting _random variables_, which will automatically be typed (due to being embedded within Haskell). We also gain the ability to _introspect_ these expressions, which will be the key requirement to build the conditioning infrastructure. We also gain the ability to _freely mix_ all haskell code with our random variables --- All of this becomes available for free thanks to the design of the library.\n",
    "\n",
    "This can be seen as a minimal implementation of [`monad-bayes`](https://github.com/adscib/monad-bayes), whose general approach we follow, but reduce the generality for brevity.\n",
    "\n",
    "Let's begin: we first enable [GADTs](https://en.wikibooks.org/wiki/Haskell/GADT), and import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE GADTs #-}\n",
    "import Control.Monad (ap, replicateM)\n",
    "import System.Random (getStdGen, getStdRandom, randomR)\n",
    "import qualified Data.Map as M\n",
    "import Control.Monad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a new type called `R`, for *random variable expression*.\n",
    "\n",
    "There are three ways to construct a random variable expression:\n",
    "\n",
    "### `Return a`\n",
    "\n",
    "`Return a` lifts a pure value `a` into a random variable, which takes on only one value `a`. This has no randomness.\n",
    "\n",
    "for example, we can construct\n",
    "\n",
    "```\n",
    "f = Return 5\n",
    "```\n",
    "to lift the pure value `5` as a random variable expression.\n",
    "\n",
    "### `Uniform f`\n",
    "\n",
    "`Uniform f` takes a callback function `f :: Float -> R a`. The input of `f` is a random sample `(s :: Float)`.  The output of `f` is  `(f s :: R a)`, which is a new *random variable expression* which might depend on the random sample `s`. `Uniform f :: R a` wraps this request as a new *random variable expression*. (This model of thinking follows the freer-monad approach, [about which one can read more here](http://okmij.org/ftp/Haskell/extensible/more.pdf))\n",
    "\n",
    "for example, we can construct\n",
    "\n",
    "```\n",
    "f = Uniform $ \\u -> -- ^ request for a random sample \n",
    "      Uniform $ \\u' -> -- ^ request for another random sample\n",
    "        Ret $ u + u' -- ^ use the responses to return a value\n",
    "```\n",
    "\n",
    "### `Weigh w r`\n",
    "\n",
    "`Weigh w r` takes as input a scaling factor `w :: Float`, and more random computations that we wish to perform as `r :: R a`. It scales the probability of the `r` computation to be executed by a factor of `w`. \n",
    "\n",
    "for example, we can construct:\n",
    "\n",
    "```\n",
    "f = Uniform $ \\u -> -- ^ request for a random *uniformly distributed* sample\n",
    "     in Weigh u (Ret $ u * u) -- ^ probability of returning value `u*u` is now scaled by `u`.\n",
    "```\n",
    "\n",
    "\n",
    "Note that while the user of the library does not need to care about the actual probabilities, the library\n",
    "implicity keeps track of the probabilities of the random samples. \n",
    "\n",
    "These random expressions `R` support a `Functor`, `Applicative`, and `Monad` instance which we implement in the following code block to easily build up random computations. We include `runRUnweighted` which shows how to run an `R a` computation to get a _random_ `a` value, without taking into account the weights (hence, `unweighted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniform random values: [0.99059993,0.53365195,0.96819735,0.83136976,0.47343558]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data R a where\n",
    "  Return :: a -> R a -- ^ lift a pure value\n",
    "  Uniform :: (Float -> R a) -- ^ computation that needs a random number to provide the rest \n",
    "                            -- of the computation\n",
    "            -> R a\n",
    "  Weigh :: Float -> R a -> R a   -- ^ scale the probability of the computation by a factor\n",
    "  \n",
    "instance Functor R where\n",
    "  fmap f (Return x) = Return (f x)\n",
    "  fmap f (Uniform rand2m) = Uniform $ \\r -> fmap f (rand2m r)\n",
    "  fmap f (Weigh w m) = Weigh w (fmap f m) \n",
    "  \n",
    "instance Monad R where\n",
    "  return = Return\n",
    "  Return a >>= f = f a\n",
    "  Uniform rand2m >>= f = Uniform $ \\r -> rand2m r >>= f\n",
    "  (Weigh w m) >>= f = Weigh w (m >>= f)\n",
    "  \n",
    "instance Applicative R where\n",
    "  pure = return\n",
    "  (<*>) = ap\n",
    "  \n",
    "-- | Run a random computation, *not taking into account the weights*.\n",
    "-- | This will be used to run the _traced computation_, which\n",
    "-- | does take into account the weights.\n",
    "runUnweighted :: R a -> IO a\n",
    "runUnweighted (Return a) = return a\n",
    "runUnweighted (Weigh w m) = runUnweighted m\n",
    "runUnweighted (Uniform rand2m) = do\n",
    "  r <- getStdRandom $ randomR (0, 1)\n",
    "  runUnweighted (rand2m r) \n",
    "  \n",
    "\n",
    "-- | A value that is uniformly distributed over (0, 1). Convenient constructor\n",
    "uniform01 :: R Float\n",
    "uniform01 = Uniform Return\n",
    "\n",
    "-- \\ Quick run of the uniform sampler to see what it outputs.\n",
    "runUnweighted (replicateM 5 uniform01) >>= \\xs -> putStrLn $ \"uniform random values: \" <> show xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing `Uniform` to build more complex random variables\n",
    "\n",
    "Now that we have a way to get random numbers _uniformly_ from the range (0, 1) we'll use this to build\n",
    "more complicated random variables --- namely, coins with different biases.\n",
    "\n",
    "`coin p` creates a coin that returns `1` with probability `p`, and `0` with probabilty `(1 - p)`.\n",
    "We pick a number `r` uniformly from `(0, 1)`. If this number is less than `p`, we return `1`, otherwise we\n",
    "return `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin 0: [0,0,0,0,0,0,0,0,0,0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin 1: [1,1,1,1,1,1,1,1,1,1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin 0.5: [0,1,1,0,1,1,0,1,1,0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- | 'coin p' returns 1 with probability p, 0 with probability (1 - p) \n",
    "coin :: Num a => Float -> R a\n",
    "coin p = do\n",
    "  r <- uniform01\n",
    "  return $ if r < p then 1 else 0\n",
    "  \n",
    "-- | Exmaple runs of the coin\n",
    "runUnweighted (replicateM 10 (coin 0)) >>= \\xs -> putStrLn $ \"coin 0: \" <> show xs\n",
    "runUnweighted (replicateM 10 (coin 1)) >>= \\xs -> putStrLn $ \"coin 1: \" <> show xs\n",
    "runUnweighted (replicateM 10 (coin 0.5)) >>= \\xs -> putStrLn $ \"coin 0.5: \" <> show xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also build `discrete`, that lets us pick a discrete value from a list of values with equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discrete: [10,100,100,10,10,100,100,100,1,1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- | Chose a discrete value with equal probability\n",
    "discrete :: [a] -> R a\n",
    "discrete as = do\n",
    "  r <- uniform01\n",
    "  let ix = floor $ r * (fromIntegral $ length as)\n",
    "  return $ as !! ix\n",
    "  \n",
    "runUnweighted (replicateM 10 (discrete [1, 10, 100])) >>= \\xs -> putStrLn $ \"discrete: \" <> show xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a tiny utility to plot many values onto the command line with fancy\n",
    "ASCII-art. This is useful when we want to sample a _large number of things_ and look at\n",
    "histograms with `histogram`, or look at the values, with `printvals`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | List of characters that represent sparklines\n",
    "sparkchars :: String\n",
    "sparkchars = \"_▁▂▃▄▅▆▇█\"\n",
    "\n",
    "-- Convert an int to a sparkline character\n",
    "num2spark :: RealFrac a => a -- ^ Max value\n",
    "  -> a -- ^ Current value\n",
    "  -> Char\n",
    "num2spark maxv curv =\n",
    "   sparkchars !!\n",
    "     (floor $ (curv / maxv) * (fromIntegral (length sparkchars - 1)))\n",
    "\n",
    "-- | Print sparklines with title\n",
    "printvals :: RealFrac a => String -> [a] -> IO ()\n",
    "printvals title vs = do \n",
    "  let maxv = maximum vs\n",
    "  putStrLn $ title ++ \" \" ++ map (num2spark maxv) vs\n",
    "  \n",
    "-- | Create a histogram from values.\n",
    "histogram :: RealFrac a\n",
    "          => String -- ^ title\n",
    "          -> Int -- ^ number of buckets\n",
    "          -> [a] -- values\n",
    "          -> IO ()\n",
    "histogram title nbuckets vs = do\n",
    "        let minv = minimum vs\n",
    "            maxv = maximum vs\n",
    "            perbucket = (maxv - minv) / (fromIntegral nbuckets)\n",
    "            bucket v = floor ((v - minv) / perbucket)\n",
    "            bucketed = M.fromListWith (+) [(bucket v, 1) | v <- vs]\n",
    "        printvals title $ M.elems $ bucketed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now draw our previous coins. a vertical bar means that we got a `1`, and not having a vertical bar\n",
    "means that we got a value `0`. We would expect `bias 0` to have no vertical bars (since we should never get a `1`). Similarly, we would expect `bias 1` to have only vertical bars (since we should always get a `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "<interactive>:1:1: error:\n    • Variable not in scope: runsUneighted :: Integer -> R Integer -> IO [Double]\n    • Perhaps you meant ‘runUnweighted’ (line 20)"
     ]
    }
   ],
   "source": [
    "-- | Print samples of coins.        \n",
    "runsUneighted 20 (coin 0) >>=  printvals \"coin: bias 0\"\n",
    "runsWeighted 20 (coin' 0.2) >>=  printvals \"coin': bias 0.2\"\n",
    "runsWeighted 20 (coin 0.5) >>=  printvals \"coin': 0.5\"\n",
    "runsWeighted 20 (coin 0.8) >>=  printvals \"coin: 0.8\"\n",
    "runsWeighted 20 (coin 1) >>=  printvals \"coin: 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why to `weigh` \n",
    "\n",
    "We now have a DSL that can describe expressions of random variables and sample from their distributions. But what is missing and essential is the ability to do _inference_: we wish to modify our distribution based on observations or arbitrary conditions. `weigh` allows us to seamlessly include conditioning into our language concisely. \n",
    "\n",
    "Besides inferece, conditioning makes our language a lot more convenient: For example, suppose we wish to describe a random variable which is the sum of two dice, where each dice lands a _prime_ value. Given `weigh`, we can express this as:\n",
    "\n",
    "```\n",
    "dice = discrete [1..6]\n",
    "sumdice = do\n",
    " d <- dice\n",
    " d' <- dice\n",
    " weigh $ if isprime d then 1 else 0\n",
    " weigh $ if isprime d' then 1 else 0\n",
    " return $ d + d'\n",
    "```\n",
    "\n",
    "Doing this without `weigh` would require us to construct this probability distribution from a uniform distribution, which is a repellent task.\n",
    "\n",
    "Let's see how `weigh` allows us to express the biased coin in a different way. We call this new implementation `coin'`. We pick `0` and `1` with equal probability by using the call `fair <- discrete[0, 1]`. We then\n",
    "_bias_ the probability of getting a `0` or a `1` with calls to `weigh`:\n",
    "\n",
    "- If `fair` returns `1`, we bias the probability by `b` by calling `weigh $ b`.\n",
    "- Similarly, if `fair` returns `0`, we bias the probability by `(1 - b)` by calling `weigh $ 1 - b`.\n",
    "\n",
    "This gives us a biased coin, but in a completely different `weigh` --- before, the computation itself returned a biased value. Now in `coin'`,  the computation is unbised, but the probability of it being executed is biased.  This can be seen by running the `coin'` with `runUnweighted`: we will see that the samples appear as if they are from a fair coin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin' 0 (will appear as fair coin, since we do not deal with weigh):  ██████_████_█_██_██__██___█_█___█_█_█___"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " -- | Change the weight of the rest of the computation. As of now, we cannot\n",
    "-- interpret this.\n",
    "weigh :: Float -> R ()\n",
    "weigh w = Weigh w (Return ())\n",
    "\n",
    " -- | A biased coin, created from a fair coin.\n",
    "coin' :: (Eq a, Num a) => Float -> R a\n",
    "coin' b = do\n",
    "  -- | pick heads or tails with uniform probability\n",
    "  fair <- discrete [0, 1]\n",
    "  -- | if the fair coin landed 1...\n",
    "  if fair == 1\n",
    "  then weigh $ b -- weigh the outcome by `b`.\n",
    "  else weigh $ (1 - b) -- otherwise weigh the outcome by `1 - b`.\n",
    "  -- | return the value that was tossed, with the new weight.\n",
    "  return fair\n",
    "\n",
    "-- | If we had weighting, we would see no vertical bars. But we do see vertical bars,\n",
    "-- | as in the case of (coin 0.5), since without weighing, `coin'` is a fair coin.\n",
    "runUnweighted (replicateM 40 (coin' 0)) \n",
    "  >>= printvals \"coin' 0 (will appear as fair coin, since we do not deal with weigh): \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to `weigh`\n",
    "\n",
    "We need the ability to take into account the calls to `weigh` we have in the code.\n",
    "For this, we use a technique described in the [Church programming language paper](https://web.stanford.edu/~ngoodman/papers/churchUAI08_rev2.pdf), and also explained in the paper [Denotational validation of higher-order bayesian inference](https://arxiv.org/abs/1711.03219). \n",
    "\n",
    "#### The high-level perspective:\n",
    "\n",
    "- in `runUnweighted`, we knew the distribution from which the random sample came from: the uniform distribution. This allowed us to sample from this distribution directly.\n",
    "\n",
    "- `Weigh` changes the probability of a value arbitrarily, and so there is no way for us to sample from the updated distribution.\n",
    "\n",
    "- The way around is to use an MCMC-like technique, where we rely on _relative weights_ of samples.\n",
    "\n",
    "- However, to perform MCMC, we need a way to propse a new sample which is close to the original sample. We only know the computation and the randomness that generated this sample. So, to propose a new value, we perturb the randomness which generated the sample to produce a new proposal sample.\n",
    "\n",
    "- We also need a way to compare the weights of these two samples. This means we need to know the total sample weight.\n",
    "\n",
    "This naturally leads us to consider _traces of computation_, which contain the value, the randomness that was used to produce this value, and the final weight of this value. We can perform metropolis-hastings on the space of _traces of computation_. We call this data structure a `Trace`.\n",
    "\n",
    "\n",
    "\n",
    "#### The `Trace` data structure:\n",
    "\n",
    "The idea is to sample from the space of \"program traces\", where a `Trace a` keeps track of:\n",
    "- The final output value --- `traceval`\n",
    "- All the randomness used in producing this output value (the list of `Float` samples that have been generated for each invocation of `Uniform`) --- `tracerands`\n",
    "- All weighting that has been done on the output value (the product of all `weigh`s found along this computational trace) --- `traceweight`.\n",
    "\n",
    "We store the traces in a `Trace a` object, and provide the functions:\n",
    "- `liftTrace` to lift a pure value into a `Trace` that uses no randomness and weight `1.0`\n",
    "- `weighTrace w t` to multiply the weight of an existing `(t :: Trace)` by `w`\n",
    "- `recordRandomnessTrace r t` to store `r` in `tracerands`. This is used to record the fact\n",
    "   that we have used this randomness, and will be used later on to \"replay\" the trace, with a perturbation.\n",
    "   \n",
    "*TODO: replace all randomness with samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | Trace of computation \n",
    "data Trace a = \n",
    "  Trace { traceval :: a -- ^ value being traced\n",
    "        , tracerands :: [Float] -- ^ all the randomness used to produce this value\n",
    "        , traceweight :: Float -- ^ weight of this current trace value\n",
    "        } deriving(Show)\n",
    "\n",
    "-- | Lift a value to a trace. Start it with weight 1.0, no randomness, and the given value\n",
    "liftTrace :: a -> Trace a\n",
    "liftTrace a = Trace a [] 1.0\n",
    "\n",
    "-- | Weigh a trace by the given weight \n",
    "weighTrace :: Float -> Trace a -> Trace a\n",
    "weighTrace w tr = tr {traceweight=(traceweight tr)*w}\n",
    "\n",
    "-- | Record the use of randomness along the trace.\n",
    "recordRandomnessTrace :: Float -> Trace a -> Trace a\n",
    "recordRandomnessTrace r tr = tr {tracerands=tracerands tr ++ [r]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement `traceR :: R a -> R (Trace a)` that allows us to record the full trace that was used to produce it.\n",
    "\n",
    "The implementation strategy is to take an expression `(r :: R a)`, and inject the correct introspection for `Ret`, `Uniform` and `Weigh`. More specifically:\n",
    "-  for a `Return x`, we use `liftTrace` to create a new traced value from a pure value\n",
    "- for a `Uniform`, we record the randomness that was passed to it with `recordRandomnessTrace`\n",
    "- for a `Weigh`, we record the weight with `weighTrace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace of coin': Trace {traceval = 0, tracerands = [0.47372633], traceweight = 0.8}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "trace of coin': Trace {traceval = 0, tracerands = [0.44166064], traceweight = 0.5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "trace of coin': Trace {traceval = 2, tracerands = [0.6901764,0.54960936], traceweight = 4.0000003e-2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- | given a regular computation, edit the computation to trace\n",
    "-- | the computation. \n",
    "traceR :: R a -> R (Trace a)\n",
    "traceR (Return x) = Return $ liftTrace x\n",
    "traceR (Uniform rand2ra) = \n",
    "  Uniform $ \\r -> do\n",
    "      -- | feed the inner computation the random value it wants,\n",
    "      -- | and continue tracing it\n",
    "      t <- traceR (rand2ra r)\n",
    "      -- | record the randomness that we used now.\n",
    "      return $ recordRandomnessTrace r t\n",
    "-- | record the weighing\n",
    "traceR (Weigh w m) = do\n",
    " t <- traceR m\n",
    " return $ (weighTrace w t)\n",
    "\n",
    "-- | Trace will have weight 0.2 or 0.8, and a single random value.\n",
    "runUnweighted (traceR (coin' 0.2)) >>= \\tr -> putStrLn $ \"trace of coin': \" <> show tr\n",
    "-- | Trace will have weight 0.5, and a single random value.\n",
    "runUnweighted (traceR (coin' 0.5)) >>= \\tr -> putStrLn $ \"trace of coin': \" <> show tr\n",
    "-- | Trace will have two random values\n",
    "runUnweighted (traceR $ liftM2 (+) (coin' 0.2) (coin' 0.2)) >>= \n",
    "  \\tr -> putStrLn $ \"trace of coin': \" <> show tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build another helper called `runRWithRandomness`. This allows us to _feed_ a sequence of pre-determined\n",
    "values to run a random computation. This allows us to perturb a trace by changing the randomness it used, and then re-running it to see the new trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin forcibly fed with randomness 0 (super unlikely to become 1): 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- RENAME THIS TO renameRWithRandomness\n",
    "-- | Run the random variable, using the randomness provided until the\n",
    "-- | randomness is exhausted\n",
    "runRWithRandomness :: [Float] -> R a -> R a\n",
    "runRWithRandomness _ (Return a) = Return a\n",
    "-- | Feed the Uniform sampling the randomness we have, and continue running\n",
    "-- with the rest of the randomness\n",
    "runRWithRandomness (r:rs) (Uniform rand2m) = runRWithRandomness rs (rand2m r)\n",
    "-- | ran out of randomnessrunRWithRandomness rs m\n",
    "runRWithRandomness [] (Uniform rand2m) = Uniform rand2m \n",
    "runRWithRandomness rs (Weigh w m) = \n",
    "  Weigh w m\n",
    "  \n",
    "-- | We feed the coin forcibly with the randomness as value 0. \n",
    "-- Note that `coin 0.0001` will be extremely unlikely to return a 1\n",
    "-- unless it is fed a random value < 0.000001.\n",
    "runUnweighted (runRWithRandomness [0] (coin 0.000001)) >>= \n",
    "  \\x -> putStrLn $ \"coin forcibly fed with randomness 0 (super unlikely to become 1): \" <> show x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build the proposal that runs a new computation given the current computation. \n",
    "\n",
    "We take the randomness that was used to produce the old trace, called `rand`. We edit the randomness at an\n",
    "index `ix` to be some arbitrary new value `r`. This new randomness is called `newrand`. We then re-run the original computation `m t` with the new randomness `newrand`. This gives us the new trace `t'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | Propose a new trace given the randomness that was used to produce\n",
    "-- the old trace\n",
    "proposeTrace :: [Float]  -- ^ randomness used to produce old trace\n",
    "  -> R (Trace a) -- ^ computation \n",
    "  -> R (Trace a) -- ^ new proposal trace.\n",
    "proposeTrace rand mt  = do\n",
    "-- | Pick a random position in the randomness of the original trace\n",
    "  ix <- discrete [0..(length $ rand) - 1]\n",
    "  -- | Edit the trace at this position by changing the randomness\n",
    "  r <- uniform01                  \n",
    "  let (randl, randr) = splitAt ix (rand)\n",
    "  -- \\ replace the randomness of the trace at this position with this\n",
    "  -- new random value, and now re-run the computation\n",
    "  let newrand = randl ++ [r] ++ drop 1 randr \n",
    "  -- | re-run the old computation, by feeding it the new randomness\n",
    "  t' <- runRWithRandomness newrand mt \n",
    "  return $ t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement `tracedMhStep`, which is a step of the metropolis-hastings over this space of traces. We propose a new trace `t'` from the randomness use to generate the old trace `t` and the computation `m t`.\n",
    "\n",
    "Our acceptance ratio is `ratio`, which is the ratio of the weights of the traces, multiplied by the ratio of the\n",
    "amounts of randomness used to produce the traces. We then perform the usual acceptance criteria of metropolis-hastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | Take samples from the traced random variable using traced monte caro\n",
    "tracedMhStep :: R (Trace a) -- ^ computation\n",
    "  -> Trace a -- ^ old trace  state\n",
    "  -> R (Trace a) -- ^ new state\n",
    "tracedMhStep mt t = do\n",
    "  -- | propose a new trace given the randomness used to produce\n",
    "  -- | the old trace.\n",
    "  t' <- proposeTrace (tracerands t) mt\n",
    "  -- | TODO: lookup \"rosenbluth factor\"\n",
    "  let ratio = traceweight t' * (fromIntegral . length . tracerands $ t') /\n",
    "               traceweight t * (fromIntegral . length . tracerands $ t)\n",
    "  accept <- uniform01\n",
    "  return $ if accept < ratio then t' else t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kick off metropolis-hastings, we need an initial trace on which we iterate over. This needs to be\n",
    "a _legal_ trace: That is, it needs to have non-zero weight. If it does not, the calculations in `tracedMhStep` will go awry: our acceptance ratios are all only sensible as long as the denominator is non-zero. The denominator will contain the weight of the inital trace. So, we just repeatedly sample from `mt` till we\n",
    "get a trace with non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | sample from the computation till we find a trace with\n",
    "-- non zero weight\n",
    "nonZeroWeightTrace :: R (Trace a) -> R (Trace a)\n",
    "nonZeroWeightTrace mt = do\n",
    "  t <- mt\n",
    "  if traceweight t == 0\n",
    "  then nonZeroWeightTrace mt\n",
    "  else return $ t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces to implement `(tracedMh :: Int -> R a -> R [a])`. Given the number of samples we want\n",
    "and the random variable, we sample from the random variable using traced metropolis hastings. We first lift the untraced computation `m` to a traced `tm = traceR m`. Then, we find a first legal trace `t = nonZeroWeightTrace tm`. Given this, we then continue to take steps of `tracedMh`, and store all the steps taken in the helper function `go`.\n",
    "\n",
    "Finally, we map `tracevals` over the list of traces `traces` to extrace out the values from the trace.\n",
    "\n",
    "We implement a small helper called `runWeighted` which first invokes `tracedMH` to build the traced MH computation, and then run this computation using `runUnweighted`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | Repeat the monadic computation n times\n",
    "loopM :: Monad m => Int -> (a -> m a) -> a -> m a\n",
    "loopM 0 _ a = return a\n",
    "loopM n f a = f a >>= loopM (n - 1) f\n",
    "\n",
    "\n",
    "-- | Take samples from a random variable by using traced metropolois hastings   \n",
    "tracedMH :: Int -> R a -> R [a]\n",
    "tracedMH n m = do\n",
    "  -- | create the traced randomness source, and sample fromt ti till we get an acceptable\n",
    "  -- | computation\n",
    "  let tm = traceR m \n",
    "  t <- nonZeroWeightTrace tm\n",
    "  -- | Int -> Trace a -> R [Trace a]\n",
    "  let go 0 t = pure []\n",
    "      go n t = do\n",
    "         t' <- loopM 10 (tracedMhStep tm) $ t\n",
    "         ts <- go (n - 1) t'\n",
    "         return $ t:ts\n",
    "  traces <- go n t\n",
    "  return $ map traceval traces\n",
    "  \n",
    "-- | get N samples from a random varaible that uses `weigh`, by sampling using traced metropolis hastings.\n",
    "runsWeighted :: Int -> R a -> IO [a]\n",
    "runsWeighted n m = runUnweighted $ tracedMH n m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payoff: `coin` vs `coin'`\n",
    "\n",
    "Let's use the machinery to run `coin'`  (recall that `coin'` was defined using `weigh`,\n",
    "and behaved like a fair coin when run with `runUnweighted`). We will run both the `coin` and `coin'` for different\n",
    "biases to check that their histograms look roughly the same.\n",
    "\n",
    "Also note that from this point onward, we will _only use_ `runsWeighted`, since our weighted sampler\n",
    "completely subsumes the unweighted sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin:  bias 0 █"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin': bias 0 █"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin:  bias 0.2 █▁"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin': bias 0.2 █▂"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin:  0.5 █▇"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin': 0.5 ▇█"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin:  0.8 ▂█"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin': 0.8 ▂█"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin:  1 █"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "coin': 1 █"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runsWeighted 100 (coin 0) >>=  histogram \"coin:  bias 0\" 2\n",
    "runsWeighted 100 (coin' 0) >>=  histogram \"coin': bias 0\" 2\n",
    "\n",
    "putStrLn \"---\"\n",
    "runsWeighted 100 (coin 0.2) >>=  histogram \"coin:  bias 0.2\" 2\n",
    "runsWeighted 100 (coin' 0.2) >>=  histogram \"coin': bias 0.2\" 2\n",
    "\n",
    "putStrLn \"---\"\n",
    "runsWeighted 100 (coin 0.5) >>=  histogram \"coin:  0.5\" 2\n",
    "runsWeighted 100 (coin' 0.5) >>=  histogram \"coin': 0.5\" 2\n",
    "\n",
    "putStrLn \"---\"\n",
    "runsWeighted 100 (coin 0.8) >>=  histogram \"coin:  0.8\" 2\n",
    "runsWeighted 100 (coin' 0.8) >>=  histogram \"coin': 0.8\" 2\n",
    "\n",
    "putStrLn \"---\"\n",
    "runsWeighted 100 (coin 1) >>=  histogram \"coin:  1\" 2\n",
    "runsWeighted 100 (coin' 1) >>=  histogram \"coin': 1\" 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, `coin` and `coin'` have similar distributions!\n",
    "\n",
    "## Sampling distributions with `weigh`:\n",
    "\n",
    "Next, we use the `weigh` mechanism to sample from _any shape of distribution we want_. The idea is this: if we want to sample points with a distribution `dist :: Float -> Float`, we will sample uniformly a value `r` in the range `(lo, hi)`, and then _weigh this `r`_ by the distribution `dist`.\n",
    "\n",
    "This allows us to sample from shapes such as:\n",
    "- $f(x) \\propto x^2 : 0 \\leq x \\leq 6$\n",
    "- $f(x) \\propto |\\sin x| : 0 \\leq x \\leq 6 $\n",
    "- $f(x) \\propto e^{-x^2} : -6 \\leq x \\leq 6$ (gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x^2 ______▁▁▁▁▂▂▂▂▂▃▄▄▆▅▅▆█▇_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "|sin x| ▂▂▅▆▆▅▇▆▆▄▃▂_▁▂▃▅▆▆█▆▇▆▄▃_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "e^{-x^2} ____▁▁▃▄▇▇▇█▇▄▂▁▁_____"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distributionToR :: (Float, Float) -- ^ support\n",
    "  -> (Float -> Float) -- ^ distribution\n",
    "  -> R Float\n",
    "distributionToR (lo, hi) dist = do\n",
    "  r <- uniform01\n",
    "  -- | take a random value uniformly distributed in (lo, hi)\n",
    "  let val = lo + r * (hi - lo)\n",
    "  -- | weigh the sample `val` with weight `d val`\n",
    "  weigh $ dist val\n",
    "  -- | return the value, with the new weight applied.\n",
    "  return $ val\n",
    "\n",
    "\n",
    "runsWeighted 1000 (distributionToR (0, 6) (^2)) >>= histogram \"x^2\" 25\n",
    "runsWeighted 1000 (distributionToR (0, 6) (abs . sin)) >>= histogram \"|sin x|\" 25\n",
    "runsWeighted 1000 (distributionToR (-6, 6) (\\x -> exp (-1.0 * x * x))) >>= histogram \"e^{-x^2}\" 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all of them work.\n",
    "\n",
    "### Inference\n",
    "\n",
    "Let's now use similar ideas to estimate the bias of a coin. We know how likely heads or tails is, given the bias of a coin. Let's call this $P(data|bias)$ (probability of the data given the bias of the coin), better known as _likelihood_. For our coin, this is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(1|bias) &= bias \\\\\n",
    "P(0|bias) &= 1 - bias\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What we want to do is solve the _inverse problem_.\n",
    "Given observations about coin flips from a coin with an _unknown bias_, we wish to _predict its bias_. That is,\n",
    "That is, we want to find its distribution $P(\\text{bias}|\\text{data})$. We solve this problem using Bayes' theorem. We know that:\n",
    "\n",
    "$$P(bias|data) = \\frac{P(data|bias) P(bias)}{P(data)}$$ \n",
    "\n",
    "The denominator is normalzation factor that is constant for a fixed $data$. Thus, we write:\n",
    "\n",
    "$$P(bias|data) \\propto P(data|bias) P(bias)$$\n",
    "\n",
    "This, if $P(bias)$ is our _prior belief_ about the biases, then $P(data|bias)$ is how much we need to multiply the\n",
    "prior with to get the _posterios belief_.\n",
    "In our case, as mentioned above, the value of $P(data|bias)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(1|bias) &= bias \\\\\n",
    "P(0|bias) &= 1 - bias\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "we implement `estimateBias` which takes a uniform prior: That is, it assumes that the bias `b` is uniformly distributed in `[0, 1]`. It then loops over all the observations `obs`. For each observation `ob`, it weighs the bias _in sequence_ by $\\texttt{likelihood} = P(data|bias)$. Finally, it returns the bias `b` that has been scaled by the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate with no data ▇▇█▇▇▅▄▂▁▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1] ▁▂▆▆▇█▇▅▃▂_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [0] ▁▄▆▇▇▇█▅▅▅_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [0, 1] ▁▄▅█▇▇▇▆▃▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0] ▁▃▅▅▇█▆▅▄▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x2 _▁▁___▂▄▆█_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x8 ___▁█_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x20 __█_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- | Given a list of observations from a coin and the bias, return a value proportional\n",
    "-- to the coin having that bias. Find this by multiplying by bias if we have a 1, (1 - bias)\n",
    "-- if we have a 0, for each heads/tails we see.\n",
    "estimateBias :: [Int] -> R Float\n",
    "estimateBias obs = do\n",
    "  b <- distributionToR (0, 2) (\\d -> exp (-1.0 * (d - 0.5) * (d - 0.5)) / 5.0) -- ^ Uniform prior\n",
    "  weigh $ if b >= 0 then 1 else 0\n",
    "  forM_ obs $ \\ob -> do\n",
    "    -- | scale by P(data|bias)\n",
    "    let likelihood = if ob == 1 then b else (1 - b)\n",
    "    weigh likelihood\n",
    "  return b\n",
    "  \n",
    "replicateList :: Int -> [a] -> [a]\n",
    "replicateList n as = mconcat $ replicate n as \n",
    "\n",
    "runsWeighted 1000 (estimateBias []) >>= histogram \"estimate with no data\" 10\n",
    "runsWeighted 1000 (estimateBias [1]) >>= histogram \"estimate with [1]\" 10\n",
    "runsWeighted 1000 (estimateBias [0]) >>= histogram \"estimate with [0]\" 10\n",
    "runsWeighted 1000 (estimateBias [0, 1]) >>= histogram \"estimate with [0, 1]\" 10\n",
    "runsWeighted 1000 (estimateBias [1, 0]) >>= histogram \"estimate with [1, 0]\" 10\n",
    "runsWeighted 1000 (estimateBias [1, 0, 1, 0]) >>= histogram \"estimate with [1, 0]x2\" 10\n",
    "runsWeighted 1000 (estimateBias (replicateList 8 [1, 0])) >>= histogram \"estimate with [1, 0]x8\" 10\n",
    "runsWeighted 1000 (estimateBias (replicateList 20 [1, 0])) >>= histogram \"estimate with [1, 0]x20\" 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as we expect: When we have no data, we have a uniform distribution over all biases. As we see data, we update our beliefs about the likely values of `bias`. For example, when we have only seen a `[1]`, we assume that it is much more likely for the coin to be unfair --- we give the fact that it has bias `1` to be the most likely.\n",
    "However, _as soon as we see more data_ in the case of `[0,1]`, we adjust our belief, and we see that we now believe that the coin is most likely fair. This allows us to _infer_ the likely bias of the coin, given the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "For me, the main takeaways from this experiment is that:\n",
    "\n",
    "- The general implementation of probabilistic programs with `Ret` and `Uniform` is well-known. These allows us to sample easily from expressions of random variables, and is very straightforward to implement in haskell. This covers the `runUnweighted` part of the story. \n",
    "\n",
    "\n",
    "- In addition to the above, the ability to _weigh_ computations is quite powerful: It lets us describe many operations more naturally than we could otherwise (see, `coin` versus `coin'`). This is much less well-known\n",
    "in the folklore (as far as I am aware), and was very interesting to learn about and implement. I originally ran across this at [`monad-bayes`](https://github.com/adscib/monad-bayes). There appears to be a rich paper-trail that one can chase, starting from `monad-bayes`.\n",
    "\n",
    "\n",
    "- Our random variable expressions come with `Functor`, `Applicative` and `Monad` instances. Throughout our implementation, we made use of the machinery that comes from implementing these typeclasses. Since these are standard typeclasses, this interacts nicely with the rest of the ecosystem. Indeed, we could get this \"for free\" [by using the free monad infrastructure](TODO: add link to free monad). This would shrink our already tiny implementation. \n",
    "\n",
    "  \n",
    "- Monte-carlo is a powerful class of techniques, which allow us to get an _approximation_ to an _arbitrary distribution_. This makes it quite unlike many of the other mathematical objects that I know. For example, in the case of groups, [deciding if two group elements are equal (the word problem) is undecidable in general](https://en.wikipedia.org/wiki/Word_problem_for_groups). I do not know of any theory of \"approximate computing of groups\".\n",
    "\n",
    "\n",
    "- What I currently dislike about this approach is needing to have the `runUnweighted`, inside which the `runWeighted` is implemented. It is useful to implement it this way, since we can have our weighted computation (`tracedMhStep`) live within the same `R` monad, and reuse the infrastructure. However, we need to introduce a `Trace` and then eliminate the `Trace`, which makes the API less elegant that what I had hoped. \n",
    "\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "- [Church, a language for generative models](https://web.stanford.edu/~ngoodman/papers/churchUAI08_rev2.pdf)\n",
    "- [Denotational validation of higher order bayesian inference](https://arxiv.org/abs/1711.03219)\n",
    "- [Practical probabilistic programming with monads](http://mlg.eng.cam.ac.uk/pub/pdf/SciGhaGor15.pdf)\n",
    "- [`monad-bayes`](https://github.com/adscib/monad-bayes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
