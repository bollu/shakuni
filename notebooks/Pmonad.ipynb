{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haskell Kōan: probabilstic programming in < 100 LoC\n",
    "\n",
    "Let us begin with our incantations to GHC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE GADTs #-}\n",
    "import Control.Monad (ap, replicateM)\n",
    "import System.Random (getStdGen, getStdRandom, randomR)\n",
    "import qualified Data.Map as M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a new monad called `P`, for *probability*.\n",
    "It supports only two operations:\n",
    "\n",
    "- `Return` converts a pure value into a `P` value,\n",
    "- `Sample` samples from a uniform distribution over [0, 1].\n",
    "\n",
    "We encode the monad in CPS style to get a monad instance for-free. For more\n",
    "modularity, use the free monad technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data P a where\n",
    "  Return :: a -> P a -- ^ lift a pure value\n",
    "  Sample :: (Float -> P a) -> P a -- ^ use a float uniformly sampled from [0, 1] \n",
    "  \n",
    "instance Functor P where\n",
    "  fmap f (Return x) = Return (f x)\n",
    "  fmap f (Sample rand2pa) = Sample $ \\r -> fmap f (rand2pa r)\n",
    "  \n",
    "instance Monad P where\n",
    "  return = Return\n",
    "  Return a >>= a2pb = a2pb a\n",
    "  Sample r2pa >>= a2pb = Sample $ \\r -> r2pa r >>= a2pb\n",
    "\n",
    "instance Applicative P where\n",
    "  pure = return\n",
    "  (<*>) = ap\n",
    "  \n",
    "sample01 :: P Float\n",
    "sample01 = Sample Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to build expressions in this language, let's build interesting objects from this.\n",
    "We start with a coin, and then we use the coin to approximate a normal distribution by adding many coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | 'coin b' returns 1 with probability p, 0 with probability (1 - p) \n",
    "coin :: Num a => Float -> P a\n",
    "coin p = do\n",
    "  r <- sample01\n",
    "  return $ if r < p then 1 else 0\n",
    "  \n",
    "-- | approximate a normal distribution using the central limit theorem by adding many\n",
    "-- uniformly distributed {0, 1} variables. Here, mean = 1\n",
    "normal :: (Fractional a, Num a) => P a \n",
    "normal = do\n",
    "  as <- replicateM 10 (coin 0.5)\n",
    "  return $ sum as / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to run our computation and plot its values, so let's build that capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | Sample a random value from a `P a`\n",
    "runP :: P a -> IO a\n",
    "runP (Return a) = return a\n",
    "runP (Sample rand2pa) = do\n",
    "  r <- getStdRandom $ randomR (0, 1)\n",
    "  runP (rand2pa r)\n",
    "\n",
    "-- | Sample `n` random values from a `P a`\n",
    "runsP :: Int -> P a -> IO [a]\n",
    "runsP n p = replicateM n (runP p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a tiny utility to plot these values onto the command line with fancy ASCII art.\n",
    "Notice that this will take us more code than everything we have done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | List of characters that represent sparklines\n",
    "sparkchars :: String\n",
    "sparkchars = \"_▁▂▃▄▅▆▇█\"\n",
    "\n",
    "-- Convert an int to a sparkline character\n",
    "num2spark :: RealFrac a => a -- ^ Max value\n",
    "  -> a -- ^ Current value\n",
    "  -> Char\n",
    "num2spark maxv curv =\n",
    "   sparkchars !!\n",
    "     (floor $ (curv / maxv) * (fromIntegral (length sparkchars - 1)))\n",
    "\n",
    "-- | Print sparklines with title\n",
    "printvals :: RealFrac a => String -> [a] -> IO ()\n",
    "printvals title vs = do \n",
    "  let maxv = if null vs then 0 else maximum vs\n",
    "  putStrLn $ title ++ \" \" ++ map (num2spark maxv) vs\n",
    "  \n",
    "  -- | Create a histogram from values.\n",
    "histogram :: RealFrac a\n",
    "          => String -- ^ title\n",
    "          -> Int -- ^ number of buckets\n",
    "          -> [a] -- values\n",
    "          -> IO ()\n",
    "histogram title nbuckets as = do\n",
    "        let minv = minimum as\n",
    "        let maxv = maximum as\n",
    "        let perbucket = (maxv - minv) / (fromIntegral nbuckets)\n",
    "        let bucket v = floor (v / perbucket)\n",
    "        let bucketed = foldl (\\m v -> M.insertWith (+) (bucket v) 1 m) mempty as\n",
    "        printvals title $ map snd . M.toList $ bucketed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces we need to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias 0 ____________________"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bias 0.2 ______█________█____"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bias 0.5 __██__████_█__█_____"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bias 0.8 __███_█████████████_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bias 1 ████████████████████"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "normal __▂▃█▁___"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runsP 20 (coin 0) >>=  printvals \"bias 0\"\n",
    "runsP 20 (coin 0.2) >>=  printvals \"bias 0.2\"\n",
    "runsP 20 (coin 0.5) >>=  printvals \"bias 0.5\"\n",
    "runsP 20 (coin 0.8) >>=  printvals \"bias 0.8\"\n",
    "runsP 20 (coin 1) >>=  printvals \"bias 1\"\n",
    "runsP 1000 normal >>= histogram \"normal\" 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can create coins, gaussians, and whatever we want _from the building block of a uniform distribution_.\n",
    "However, we still don't have a way to take samples from _arbitrary distributions_.\n",
    "What if we want to sample from a distribution of the form `p(x) = x^{-2}`, or `p(x) = sin(x)`?\n",
    "Enter MCMC (markov-chain-monte-carlo), a class of powerful techniques that allow us to sample from any choice of distribution.\n",
    "\n",
    "The part of this that really tickles my fancy is that the MCMC algorithm we are going to implement\n",
    "(metropolis-hastings) _is described within our `P` monad_.\n",
    "\n",
    "That is, we have a constructive proof that given the ability to sample from a uniform distribution,\n",
    "we can sample from _any_ (computable) distribution. I find this quite fact\n",
    "really marvellous -- it is rare that mathematical objects behave this well with respect\n",
    "to computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | take a sampler and create a new probabilstic value, which samples from the original\n",
    "-- based on the score values (Metropolis-Hastings)\n",
    "mhStep :: (a -> Float) -- ^ score function\n",
    "   -> (a -> P a) -- ^ proposal: given a value, provide values around that value\n",
    "   -> a -- ^ current value\n",
    "   -> P a -- ^ next value\n",
    "mhStep scoring proposer a = do\n",
    "   a' <- proposer a\n",
    "   let accept = scoring a' / scoring a\n",
    "   u <- sample01\n",
    "   return $ if u < accept then a' else a\n",
    "\n",
    "-- | Take N steps of metropolois-hastings\n",
    "mhSteps :: Int -> (a -> Float) -> (a -> P a) -> a -> P a\n",
    "mhSteps 0 _ _ a = return a\n",
    "mhSteps n scoring proposer a = mhStep scoring proposer a >>= mhSteps (n - 1) scoring proposer\n",
    "\n",
    "-- | A way to sample uniformly from floats in a given range\n",
    "sampleFloatUniform :: Float -> Float ->  P Float\n",
    "sampleFloatUniform l h = do\n",
    "   u <- sample01\n",
    "   return $ l + u * (h - l)\n",
    "  \n",
    "-- | Generate samples for a numeric type printvals given a scoring function\n",
    "-- using metropolis hastings\n",
    "mhFloat :: (Float, Float) -> (Float -> Float) -> P Float\n",
    "mhFloat (lo, hi) scorer = mhSteps 10 scorer (const $ sampleFloatUniform lo hi) 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check if this works! If it does work, then our histograms should look like the functions that we are plotting, since the function we pass to `mhFloat` is the probability density that we wish to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniform ▆▅▆▆▆▅▇█▅▅▆▆▇▆▆▄▆▅▆▇_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x^2 _____▁▁▁▂▂▂▂▃▃▄▅▅▆▆█▇"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "|sin x| _▂▄▅▆█▆▄▃▂_▁▃▆▅▅▆▆▃▂_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "|cos x| █▇▅▄▁_▂▅▇▇▇▇▆▄▂_▁▂▆▆_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runsP 1000 (mhFloat (0, 6) (const 1.0)) >>= histogram \"uniform\" 20\n",
    "runsP 1000 (mhFloat (0, 6) (^2)) >>= histogram \"x^2\" 20\n",
    "runsP 1000 (mhFloat (0, 6) (abs . sin)) >>= histogram \"|sin x|\" 20\n",
    "runsP 1000 (mhFloat (0, 6) (abs . cos)) >>= histogram \"|cos x|\" 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all of them seem to work. Let's now use similar ideas to estimate the bias of a coin. The idea is as follows:\n",
    "\n",
    "We have a model of a coin, and we know how likely heads or tails is given the model of a coin. Let's call this\n",
    "$P(d|m)$ (probability of the $d$ata given the $m$odel). This is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(1|\\text{bias}) &= \\text{bias} \\\\\n",
    "P(0|\\text{bias}) &= 1 - \\text{bias}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What we want to do is the _inverse problem_.\n",
    "Given observations about coin flips from a coin with an _unknown bias_, we wish to _predict its bias_.\n",
    "That is, we want to find $P(\\text{bias}|\\text{data})$.\n",
    "\n",
    "We solve this problem using Bayes' theorem. We know that\n",
    "\n",
    "$$P(bias|data) = \\frac{P(data|bias) P(bias)}{P(data)}$$ \n",
    "\n",
    "The denominator is normalzation factor that is constant for a fixed dataset. Thus, we write:\n",
    "\n",
    "$$P(bias|data) \\propto P(data|bias) P(bias)$$\n",
    "\n",
    "This, if $P(bias)$ is our _prior belief_ about the biases, then $P(data|bias)$ is how much we need to multiply the\n",
    "prior with to get the _posterios belief_.\n",
    "\n",
    "In our case, as mentioned above, the value of $P(data|bias)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(1|\\text{bias}) &= \\text{bias} \\\\\n",
    "P(0|\\text{bias}) &= 1 - \\text{bias}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate with no data ▇▄▅▇██▅▆▇▆▆▅▇▅▇▆▇▆▅▅_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1] _▁▁▁▂▂▂▃▄▄▃▄▆▆▆▇▇▆▆█▇"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [0] ▇█▇▆▆▅▅▅▄▄▂▃▃▂▂▁▁▁___"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [0, 1] _▂▂▃▄▄▆▆▅█▇▇▅▇▅▄▃▃▂▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0] _▂▃▄▃▆▆▇▇▇▆▇█▇▆▅▅▃▃▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x2 __▁▂▄▄▅▅▇▆▇▇█▇▆▅▄▃▂▁_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x8 ____▁▂▃▄▄▆█▆▄▄▂▁____"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "estimate with [1, 0]x20 ____▁▁▂▃▃█▃▃▃▂▁____"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- | Given a list of observations from a coin and the bias, return a value proportional\n",
    "-- to the coin having that bias. Find this by multiplying by bias if we have a 1, (1 - bias)\n",
    "-- if we have a 0, for each heads/tails we see.\n",
    "estimateBias :: [Int] -> Float -> Float\n",
    "estimateBias obs bias = \n",
    "  product $ (map (\\o -> if o == 1 then bias else (1 - bias)) obs)\n",
    "\n",
    "replicateList :: Int -> [a] -> [a]\n",
    "replicateList n as = mconcat $ replicate n as \n",
    "\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias []) >>= histogram \"estimate with no data\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias [1]) >>= histogram \"estimate with [1]\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias [0]) >>= histogram \"estimate with [0]\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias [0, 1]) >>= histogram \"estimate with [0, 1]\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias [1, 0]) >>= histogram \"estimate with [1, 0]\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias [1, 0, 1, 0]) >>= histogram \"estimate with [1, 0]x2\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias (replicateList 8 [1, 0])) >>= histogram \"estimate with [1, 0]x8\" 20\n",
    "runsP 1000 (mhFloat (0, 1) $ estimateBias (replicateList 20 [1, 0])) >>= histogram \"estimate with [1, 0]x20\" 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the \"estimate with no data\" is roughly uniform: we knew nothing, so we assume that every event is equally likely.\n",
    "\n",
    "as we gain more data, we update our beliefs about what the bias of the coin is. For example, if we see _just a single `[1]`_, we think it's much more likely for the coin to be biased. \n",
    "\n",
    "However, as soon as we see another data point as `[1, 0]`, we adjust our beliefs drastically: we now think the coin is most likely fair, since in two tosses of the coin, we saw once a heads and then a tails.\n",
    "\n",
    "If one wishes to, we could link this update to notions of [maximum likelihood estimation](TODO: fill link), but we won't be pursuing this right now here. \n",
    "\n",
    "In this library, we have a duality of representations: For one, we are able to represent _random variables_ such as that of the biased coin. We think of `P Int` as a \"random integer variable\". On the other hand, for the bias estimation example, we live in the world of _probability distributions_: We think directly of the probabilities of the bias of the coin.\n",
    "\n",
    "There are different ways of unifying this approach. One way of approaching this would be as the one [`monad-bayes`](TODO: fill link) takes, which is to add a `score` function into the `P` monad to multiply the distribution with some weight.  While this approach works, and we draw a lot of inspiration from their implementation, it still feels insufficient --- We should either live in the world of random variables, or in the world of distributions, and not mix these two worlds up. As Conal Elliot often says, \"this lacks the feeling of _inevatibility_\".  I would love to hear about library design ideas that manages to separate the two worlds cleanly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell - shakuni",
   "language": "haskell",
   "name": "ihaskell_shakuni"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
